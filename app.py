import os
from pathlib import Path
from datetime import datetime
import json
import numpy as np
import pandas as pd
import streamlit as st
import joblib
import shap
from openai import OpenAI

# -------------------------------
# í˜ì´ì§€/í‚¤/ëª¨ë¸ ê²½ë¡œ
# -------------------------------
st.set_page_config(page_title="ğŸ–¥ï¸ KCD 2025 J. - Will the first extubation be successful?", layout="wide")

OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", None)
OPENAI_BASE_URL = st.secrets.get("OPENAI_BASE_URL", None)

USE_LLM = True
FIXED_TAGGING_MODEL = "gpt-4o-mini"
FIXED_GENERATION_MODEL = "gpt-4o-mini"
CHAT_MODEL = "gpt-4o-mini"

# âœ… ì˜¨í†¨ë¡œì§€ í¬í•¨ í•™ìŠµ ëª¨ë¸
FIXED_MODEL_PATH = "./models/best_model_medgemma.pkl"

# -------------------------------
# í”¼ì²˜ ìŠ¤í‚¤ë§ˆ
# -------------------------------
REQUIRED_FEATURES = [
    'AGE', 'SEX', 'BMI', 'VENT_DUR', 'CHF', 'CVD', 'CPD', 'CKD', 'CLD',
    'DM', 'ECMO', 'CRRT', 'MAP', 'HR', 'RR', 'BT', 'SPO2', 'GCS', 'PH',
    'PACO2', 'PAO2', 'HCO3', 'LACTATE', 'WBC', 'HB', 'PLT', 'SODIUM',
    'POTASSIUM', 'CHLORIDE', 'BUN', 'CR', 'PT', 'FIO2', 'PEEP', 'PPLAT', 'TV'
]

# âœ… ì˜¨í†¨ë¡œì§€ 10ê°œ (ëª¨ë¸ ì…ë ¥ì—ë„ í¬í•¨ë¨)
ONTO_FEATURES = [
    "diabetes_mellitus",
    "obesity",
    "prolonged_mechanical_ventilation_history",
    "advanced_age",
    "low_PaO2_FiO2_ratio",
    "congestive_heart_failure",
    "anemia",
    "hemodynamic_instability",
    "low_mean_arterial_pressure",
    "leukocytosis",
]

ALL_FEATURES_FALLBACK = REQUIRED_FEATURES + ONTO_FEATURES

# -------------------------------
# ì˜ˆì‹œ ì¼€ì´ìŠ¤
# -------------------------------
EXAMPLE_CASES = {
    "ì¼€ì´ìŠ¤ 1": {'AGE': 60.0,'SEX': 0.0,'BMI': 31.49090228,'VENT_DUR': 24.88333333,'CHF': 0.0,'CVD': 1.0,'CPD': 0.0,'CKD': 0.0,'CLD': 0.0,'DM': 1.0,'ECMO': 0.0,'CRRT': 0.0,'MAP': 65.0,'HR': 86.0,'RR': 20.0,'BT': 36.5,'SPO2': 95.0,'GCS': 9.0,'PH': 7.41,'PACO2': 40.0,'PAO2': 136.0,'HCO3': 23.0,'LACTATE': 1.8,'WBC': 13.3,'HB': 11.2,'PLT': 166.0,'SODIUM': 138.0,'POTASSIUM': 4.1,'CHLORIDE': 105.0,'BUN': 10.0,'CR': 0.7,'PT': 12.2,'FIO2': 40.0,'PEEP': 5.0,'PPLAT': 21.0,'TV': 546.0},
    "ì¼€ì´ìŠ¤ 2": {'AGE': 72.0,'SEX': 1.0,'BMI': 27.3,'VENT_DUR': 72.0,'CHF': 1.0,'CVD': 0.0,'CPD': 1.0,'CKD': 0.0,'CLD': 0.0,'DM': 0.0,'ECMO': 0.0,'CRRT': 0.0,'MAP': 58.0,'HR': 128.0,'RR': 28.0,'BT': 38.4,'SPO2': 88.0,'GCS': 8.0,'PH': 7.32,'PACO2': 50.0,'PAO2': 70.0,'HCO3': 24.0,'LACTATE': 3.2,'WBC': 18.5,'HB': 10.1,'PLT': 140.0,'SODIUM': 136.0,'POTASSIUM': 4.8,'CHLORIDE': 104.0,'BUN': 26.0,'CR': 1.2,'PT': 14.0,'FIO2': 60.0,'PEEP': 8.0,'PPLAT': 28.0,'TV': 420.0},
    "ì¼€ì´ìŠ¤ 3": {'AGE': 45.0,'SEX': 1.0,'BMI': 33.8,'VENT_DUR': 12.0,'CHF': 0.0,'CVD': 0.0,'CPD': 0.0,'CKD': 0.0,'CLD': 0.0,'DM': 0.0,'ECMO': 0.0,'CRRT': 0.0,'MAP': 75.0,'HR': 92.0,'RR': 18.0,'BT': 36.8,'SPO2': 97.0,'GCS': 13.0,'PH': 7.43,'PACO2': 37.0,'PAO2': 120.0,'HCO3': 24.0,'LACTATE': 1.2,'WBC': 9.8,'HB': 12.5,'PLT': 210.0,'SODIUM': 140.0,'POTASSIUM': 3.9,'CHLORIDE': 103.0,'BUN': 12.0,'CR': 0.8,'PT': 12.0,'FIO2': 35.0,'PEEP': 5.0,'PPLAT': 19.0,'TV': 500.0}
}

# -------------------------------
# í´ë¼ì´ì–¸íŠ¸/ëª¨ë¸ ë¡œë”
# -------------------------------
def build_openai_client():
    if OPENAI_API_KEY is None:
        return None
    if OPENAI_BASE_URL:
        return OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
    return OpenAI(api_key=OPENAI_API_KEY)

@st.cache_resource(show_spinner=False)
def load_model(model_path: str = FIXED_MODEL_PATH):
    p = Path(model_path)
    if not p.exists():
        raise FileNotFoundError(f"Model not found at {model_path}.")
    model = joblib.load(str(p))  # RandomForest or Pipeline with ColumnTransformer
    return model

def _df_from_patient_input(patient_input: dict) -> pd.DataFrame:
    df = pd.DataFrame([patient_input])
    df = df[REQUIRED_FEATURES]
    return df

# -------------------------------
# ì˜¨í†¨ë¡œì§€ ë¼ë²¨/ì„¤ëª…
# -------------------------------
def _ontology_label_maps():
    labels = {
        "diabetes_mellitus": "ë‹¹ë‡¨ë³‘",
        "obesity": "ë¹„ë§Œ",
        "prolonged_mechanical_ventilation_history": "ì¥ê¸°ê°„ ê¸°ê³„í™˜ê¸° ë³‘ë ¥",
        "advanced_age": "ê³ ë ¹",
        "low_PaO2_FiO2_ratio": "ë‚®ì€ PaO2/FiO2 ë¹„ìœ¨",
        "congestive_heart_failure": "ìš¸í˜ˆì„± ì‹¬ë¶€ì „",
        "anemia": "ë¹ˆí˜ˆ",
        "hemodynamic_instability": "í˜ˆì—­í•™ì  ë¶ˆì•ˆì •",
        "low_mean_arterial_pressure": "ë‚®ì€ í‰ê·  ë™ë§¥ì••",
        "leukocytosis": "ë°±í˜ˆêµ¬ ì¦ê°€ì¦",
    }
    desc = {
        "diabetes_mellitus": "ë‹¹ë‡¨ë³‘ ë³‘ë ¥ ë˜ëŠ” í˜ˆë‹¹ ì¡°ì ˆ ë¬¸ì œ",
        "obesity": "BMIâ‰¥30",
        "prolonged_mechanical_ventilation_history": "ê¸°ê³„í™˜ê¸° 48ì‹œê°„ ì´ìƒ",
        "advanced_age": "ë‚˜ì´â‰¥65ì„¸",
        "low_PaO2_FiO2_ratio": "PaO2/FiO2<200",
        "congestive_heart_failure": "ìš¸í˜ˆì„± ì‹¬ë¶€ì „ ë³‘ë ¥",
        "anemia": "Hb<10 g/dL",
        "hemodynamic_instability": "HR<40/HR>120 ë˜ëŠ” MAP<60",
        "low_mean_arterial_pressure": "MAP<65",
        "leukocytosis": "WBC>12 (Ã—10^3/Î¼L)",
    }
    return labels, desc

def summarize_ontology_for_report(ontology_json: dict):
    labels, desc = _ontology_label_maps()
    row = ontology_json["patients"][0]
    positives, negatives = [], []
    for k in labels:
        item = {
            "key": k,
            "name": labels[k],
            "rule": desc[k],
            "value": int(row.get(k, 0))
        }
        (positives if item["value"] == 1 else negatives).append(item)
    return positives, negatives

def ontology_pretty_table(ontology_json: dict) -> pd.DataFrame:
    labels, desc = _ontology_label_maps()
    row = ontology_json["patients"][0]
    rows = []
    for k in labels:
        val = int(row.get(k, 0))
        icon = "âœ…" if val == 1 else "âŒ"
        rows.append({"íŠ¹ì„±": labels[k], "ì„¤ëª…": desc[k], "ì—¬ë¶€": icon})
    return pd.DataFrame(rows)

# -------------------------------
# ì˜¨í†¨ë¡œì§€ íƒœê¹… (LLM/ë£°)
# -------------------------------
def rule_based_ontology(df: pd.DataFrame) -> dict:
    row = df.iloc[0]
    dm = int(row.get("DM", 0) == 1)
    obesity = int(float(row.get("BMI", 0)) >= 30)
    prolonged_mv = int(float(row.get("VENT_DUR", 0)) >= 48)
    advanced_age = int(float(row.get("AGE", 0)) >= 65)

    pao2 = float(row.get("PAO2", 0))
    fio2_pct = float(row.get("FIO2", 21.0))
    fio2_frac = max(fio2_pct, 21.0) / 100.0
    pf_ratio = pao2 / fio2_frac if fio2_frac > 0 else 0.0
    low_pfr = int(pf_ratio < 200)

    chf = int(row.get("CHF", 0) == 1)
    anemia = int(float(row.get("HB", 0)) < 10.0)
    hemo_instab = int((float(row.get("HR", 80)) < 40) or (float(row.get("HR", 80)) > 120) or (float(row.get("MAP", 80)) < 60))
    low_map = int(float(row.get("MAP", 80)) < 65)
    leukocytosis = int(float(row.get("WBC", 0)) > 12.0)

    return {
        "patients": [
            {
                "patient_index": 0,
                "diabetes_mellitus": dm,
                "obesity": obesity,
                "prolonged_mechanical_ventilation_history": prolonged_mv,
                "advanced_age": advanced_age,
                "low_PaO2_FiO2_ratio": low_pfr,
                "congestive_heart_failure": chf,
                "anemia": anemia,
                "hemodynamic_instability": hemo_instab,
                "low_mean_arterial_pressure": low_map,
                "leukocytosis": leukocytosis,
            }
        ]
    }

def llm_tag_ontology(client: OpenAI, df: pd.DataFrame) -> dict:
    patient_records = df.to_dict(orient='records')
    schema = {
        "patients": [{
            "patient_index": 0,
            "diabetes_mellitus": 0,
            "obesity": 0,
            "prolonged_mechanical_ventilation_history": 0,
            "advanced_age": 0,
            "low_PaO2_FiO2_ratio": 0,
            "congestive_heart_failure": 0,
            "anemia": 0,
            "hemodynamic_instability": 0,
            "low_mean_arterial_pressure": 0,
            "leukocytosis": 0
        }]
    }
    prompt = (
        "ë‹¹ì‹ ì€ ì˜ë£Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í™˜ì ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë°œê´€(extubation) ì‹œ ìœ„í—˜ ìš”ì¸ì´ ë  ìˆ˜ ìˆëŠ” "
        "ì˜¨í†¨ë¡œì§€ íŠ¹ì„±ì„ íƒœê¹…í•´ì£¼ì„¸ìš”.\n\n"
        f"í™˜ì ë°ì´í„°:\n{json.dumps(patient_records, ensure_ascii=False, indent=2)}\n\n"
        "ë‹¤ìŒ íŠ¹ì„±ì„ 0 ë˜ëŠ” 1ë¡œ ë°˜í™˜:\n"
        "1. diabetes_mellitus\n2. obesity\n3. prolonged_mechanical_ventilation_history\n"
        "4. advanced_age\n5. low_PaO2_FiO2_ratio\n6. congestive_heart_failure\n7. anemia\n"
        "8. hemodynamic_instability\n9. low_mean_arterial_pressure\n10. leukocytosis\n\n"
        "ë°˜ë“œì‹œ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆë§Œ ë°˜í™˜:\n"
        f"{json.dumps(schema, ensure_ascii=False, indent=2)}"
    )
    resp = client.chat.completions.create(
        model=FIXED_TAGGING_MODEL,
        messages=[
            {"role":"system","content":"ì˜ë£Œ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ JSONë§Œ ë°˜í™˜í•©ë‹ˆë‹¤."},
            {"role":"user","content":prompt}
        ],
        temperature=0.2,
        response_format={"type":"json_object"}
    )
    return json.loads(resp.choices[0].message.content)

def attach_ontology_features(df: pd.DataFrame, ontology_json: dict):
    row = ontology_json["patients"][0]
    for k in ONTO_FEATURES:
        df[k] = int(row.get(k, 0))
    return df

# -------------------------------
# ëª¨ë¸ ê¸°ëŒ€ í”¼ì²˜ ìë™ ì¶”ë¡ 
# -------------------------------
def get_expected_model_features(model, fallback_cols):
    # 1) feature_names_in_
    if hasattr(model, "feature_names_in_"):
        return list(model.feature_names_in_)
    # 2) íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ ì¶”ë¡ 
    try:
        from sklearn.pipeline import Pipeline
        from sklearn.compose import ColumnTransformer
        if isinstance(model, Pipeline):
            for name, step in model.steps:
                if hasattr(step, "feature_names_in_"):
                    return list(step.feature_names_in_)
                if isinstance(step, ColumnTransformer) and hasattr(step, "feature_names_in_"):
                    return list(step.feature_names_in_)
    except Exception:
        pass
    # 3) ì‹¤íŒ¨ ì‹œ í´ë°±(ì›ë³¸+ì˜¨í†¨ë¡œì§€)
    return list(fallback_cols)

# -------------------------------
# ì˜ˆì¸¡ / SHAP
# -------------------------------
def run_predict(model, df_model: pd.DataFrame):
    # DataFrame ìš°ì„ 
    try:
        proba = model.predict_proba(df_model)
    except Exception:
        proba = model.predict_proba(df_model.values)

    pos_idx = 1
    if hasattr(model, "classes_"):
        classes = list(model.classes_)
        if 1 in classes:
            pos_idx = classes.index(1)

    p = float(proba[0, pos_idx])
    y = "ìœ„í—˜" if p > 0.5 else "ì•ˆì „"
    return {"probability": p, "class_label": y}

def compute_shap(model, df_model: pd.DataFrame):
    """
    ì•ˆì •í˜• SHAP: KernelExplainerë¡œ class1 í™•ë¥ ì— ëŒ€í•œ ê¸°ì—¬ë„ë¥¼ ê³„ì‚°.
    íŒŒì´í”„ë¼ì¸/í¬ì†Œí–‰ë ¬/íŠ¸ë¦¬í•´ì„ê¸° ë¶ˆì¼ì¹˜ ë¬¸ì œë¥¼ íšŒí”¼.
    """
    # class1 ì¸ë±ìŠ¤
    pos_idx = 1
    if hasattr(model, "classes_"):
        classes = list(model.classes_)
        if 1 in classes:
            pos_idx = classes.index(1)

    # ì˜ˆì¸¡ í•¨ìˆ˜: class1 í™•ë¥  ë°˜í™˜
    def pred_fn(X):
        X_df = pd.DataFrame(X, columns=df_model.columns)
        try:
            proba = model.predict_proba(X_df)
        except Exception:
            proba = model.predict_proba(X_df.values)
        return proba[:, pos_idx]

    # ë°±ê·¸ë¼ìš´ë“œ: ë™ì¼ ìƒ˜í”Œ ë³µì œ(ì†ë„/ì•ˆì • ê· í˜•)
    bg = df_model.to_numpy()
    if len(bg) < 10:
        bg = np.vstack([bg] * 10)

    explainer = shap.KernelExplainer(pred_fn, bg)
    shap_arr = explainer.shap_values(df_model.to_numpy(), nsamples=100)
    shap_arr = np.asarray(shap_arr)
    sample_vals = shap_arr[0]  # 1D (n_features,)

    names = df_model.columns.tolist()
    imp_dict = {names[i]: float(np.asarray(sample_vals[i]).reshape(())) for i in range(len(names))}
    top5 = sorted([(k, abs(v)) for k, v in imp_dict.items()], key=lambda x: x[1], reverse=True)[:5]

    return {"shap_values": shap_arr, "feature_importance": imp_dict, "top_risk_factors": top5}

# -------------------------------
# ë ˆí¬íŠ¸ ìƒì„±
# -------------------------------
def summarize_shap_for_report(shap_exp: dict, top_k: int = 5):
    fi = shap_exp.get("feature_importance", {}) or {}
    if not fi:
        return [], []
    triples = [(name, abs(val), val) for name, val in fi.items()]
    triples.sort(key=lambda x: x[1], reverse=True)
    top = [{"feature": n, "abs_importance": round(a, 5), "direction": ("ìœ„í—˜ ì¦ê°€" if s > 0 else "ìœ„í—˜ ê°ì†Œ")}
           for (n, a, s) in triples[:top_k]]
    bottom = [{"feature": n, "abs_importance": round(a, 5), "direction": ("ìœ„í—˜ ì¦ê°€" if s > 0 else "ìœ„í—˜ ê°ì†Œ")}
              for (n, a, s) in triples[-top_k:]]
    return top, bottom

def llm_generate_report(client: OpenAI, patient_input: dict, prediction: dict, shap_exp: dict, ontology_json: dict) -> str:
    ont_pos, ont_neg = summarize_ontology_for_report(ontology_json)
    shap_top, _ = summarize_shap_for_report(shap_exp, top_k=5)

    ont_pos_for_llm = [{"name": x["name"], "rule": x["rule"]} for x in ont_pos] or [{"name": "í•´ë‹¹ í•­ëª© ì—†ìŒ", "rule": ""}]
    ont_neg_for_llm = [{"name": x["name"], "rule": x["rule"]} for x in ont_neg]
    shap_top_for_llm = shap_top or [{"feature": "í•´ë‹¹ ì—†ìŒ", "abs_importance": 0.0, "direction": ""}]
    cls_label = prediction.get("class_label", "ì•ˆì „")

    prompt = (
        "ë‹¹ì‹ ì€ ì˜ë£Œì§„ê³¼ í™˜ì ë³´í˜¸ì ì‚¬ì´ì˜ ì†Œí†µì„ ë•ëŠ” ì˜ë£Œ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
        "ì•„ë˜ 'ëª¨ë¸ ì˜ˆì¸¡', 'ì˜¨í†¨ë¡œì§€ íŒë‹¨ ê²°ê³¼', 'SHAP ì¤‘ìš” ìš”ì¸'ì„ ëª¨ë‘ ì°¸ê³ í•˜ì—¬, "
        "ë³´í˜¸ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª… ë ˆí¬íŠ¸ë¥¼ ì˜ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
        f"[ëª¨ë¸ ì˜ˆì¸¡]\n- ë°œê´€ ì‹¤íŒ¨ í™•ë¥ : {prediction['probability']:.1%}\n- ì˜ˆì¸¡ í´ë˜ìŠ¤(ì•ˆì „/ìœ„í—˜): {cls_label}\n\n"
        f"[ì˜¨í†¨ë¡œì§€ íŒë‹¨ ê²°ê³¼]\n- ì´ í™˜ìì—ê²Œ ì‹¤ì œ í•´ë‹¹ëœ í•­ëª©(ê°’=1): {json.dumps(ont_pos_for_llm, ensure_ascii=False)}\n"
        f"- í•´ë‹¹ë˜ì§€ ì•Šì€ í•­ëª©(ê°’=0): {json.dumps(ont_neg_for_llm, ensure_ascii=False)}\n\n"
        f"[SHAP ì¤‘ìš” ìš”ì¸ Top 5]\n- {json.dumps(shap_top_for_llm, ensure_ascii=False)}\n\n"
        f"[ì…ë ¥ ë°ì´í„°(ìš”ì•½)]\n- ì¼ë¶€ ì£¼ìš” ìˆ˜ì¹˜: "
        f"{json.dumps({k: patient_input[k] for k in ['AGE','BMI','SPO2','MAP','HR','RR','GCS','PH','PACO2','PAO2','HCO3','LACTATE','FIO2','PEEP','PPLAT','TV'] if k in patient_input}, ensure_ascii=False)}\n\n"
        "[ì‘ì„± ì§€ì¹¨]\n"
        "- í†¤&ìŠ¤íƒ€ì¼: ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ì„¸ìš”. ë³´í˜¸ìì˜ ë¶ˆì•ˆê°ì„ ì´í•´í•˜ë©´ì„œë„ ì •í™•í•œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì„¸ìš”. ì˜í•™ì  ì „ë¬¸ì„±ê³¼ ì¸ê°„ë¯¸ë¥¼ ê· í˜• ìˆê²Œ í‘œí˜„í•˜ì„¸ìš”."
        "- ì˜¨í†¨ë¡œì§€ì—ì„œ ê°’=1ì¸ í•­ëª©ì€ 'ì´ í™˜ìì—ê²Œ ì‹¤ì œë¡œ ê´€ì°°ëœ ìœ„í—˜ ì‹ í˜¸'ë¡œ ë°˜ë“œì‹œ ë³¸ë¬¸ì— í¬í•¨í•˜ì„¸ìš”.\n"
        "- SHAP ìƒìœ„ ìš”ì¸ì€ 'ì™œ ì´ëŸ° ì˜ˆì¸¡ì´ ë‚˜ì™”ëŠ”ì§€' ì„¤ëª…í•˜ëŠ” ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. (ì¦ê°€/ê°ì†Œ ë°©í–¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì„œìˆ )\n"
        "- ê°’=0ì¸ ì˜¨í†¨ë¡œì§€ í•­ëª©ì€ í•„ìš” ì‹œ 'ì™„í™” ìš”ì¸' ë˜ëŠ” 'í˜„ì¬ëŠ” í•´ë‹¹ ì—†ìŒ'ìœ¼ë¡œ ê°„ë‹¨íˆ ì–¸ê¸‰í•´ë„ ë©ë‹ˆë‹¤.\n"
        "- í™•ë¥  ìˆ˜ì¹˜ëŠ” ë¹„ìœ /ì‚¬ë¡€ë¡œ ì‰½ê²Œ ì„¤ëª…í•˜ë˜, ì ˆëŒ€ì ì¸ ì§„ë‹¨ì´ ì•„ë‹˜ì„ ë¶„ëª…íˆ í•˜ì„¸ìš”.\n"
        "- ì „ë¬¸ìš©ì–´ëŠ” ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.\n"
        "- ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€, ì„¹ì…˜ ì œëª©ì€ [ì„¹ì…˜ëª…] í˜•íƒœ.\n"
        "- ë¶„ëŸ‰: 8~14ë¬¸ì¥ ì •ë„.\n\n"
        "[ê¶Œì¥ êµ¬ì„±]\n"
        "1) [ì¸ì‚¬ ë° ëª©ì ] 2~3ë¬¸ì¥\n"
        "2) [í˜„ì¬ ìƒíƒœ ìš”ì•½] 2~3ë¬¸ì¥ (ì¤‘ìš” ìˆ˜ì¹˜ ê°„ë‹¨ í•´ì„)\n"
        "3) [ì˜ˆì¸¡ ê²°ê³¼ í•´ì„] 2~3ë¬¸ì¥ (í™•ë¥  ì˜ë¯¸, ì•ˆì „/ìœ„í—˜ ë§¥ë½)\n"
        "4) [í•´ë‹¹ëœ ìœ„í—˜ ì‹ í˜¸(ì˜¨í†¨ë¡œì§€)] 2~4ë¬¸ì¥ (ê°’=1 í•­ëª©ì„ í’€ì–´ì„œ ì„¤ëª…)\n"
        "5) [ì˜ˆì¸¡ ê·¼ê±°(ëª¨ë¸ ê´€ì )] 2~3ë¬¸ì¥ (SHAP Top ìš”ì¸ì„ ì‰¬ìš´ ì–¸ì–´ë¡œ)\n"
        "6) [ê¶Œê³  ë° ë§ˆë¬´ë¦¬] 1~2ë¬¸ì¥ (ëª¨ë‹ˆí„°ë§/ì†Œí†µ ê°•ì¡°)\n"
    )

    resp = client.chat.completions.create(
        model=FIXED_GENERATION_MODEL,
        messages=[
            {"role":"system","content":"ë‹¹ì‹ ì€ ì˜ë£Œ ì •ë³´ë¥¼ ì¼ë°˜ì¸ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì „ë‹¬í•˜ëŠ” ì˜ë£Œ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role":"user","content":prompt}
        ],
        temperature=0.5
    )

    body = resp.choices[0].message.content.strip()
    header = f"{'='*60}\nê¸°ê³„í™˜ê¸° ë°œê´€ ì•ˆë‚´ë¬¸\n{'='*60}\n\nìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M')}\n\n"
    footer = f"\n\n{'='*60}\në³¸ ì•ˆë‚´ë¬¸ì€ AI ê¸°ë°˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œì„ í™œìš©í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\nìµœì¢… ì˜ë£Œ ê²°ì •ì€ ë‹´ë‹¹ ì˜ë£Œì§„ì˜ ì¢…í•©ì ì¸ íŒë‹¨ì— ë”°ë¼ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\nê¶ê¸ˆí•œ ì ì´ë‚˜ ìš°ë ¤ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì˜ë£Œì§„ì—ê²Œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.\n{'='*60}\n"
    return header + body + footer

# -------------------------------
# ì„¸ì…˜ ìƒíƒœ
# -------------------------------
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "memory" not in st.session_state:
    st.session_state.memory = {}

# -------------------------------
# UI
# -------------------------------
st.title("ğŸ–¥ï¸ KCD 2025 J. - Will the first extubation be successful?")

st.subheader("â¡ï¸ Select Example Case")
selected_case = st.selectbox("Example Case", list(EXAMPLE_CASES.keys()), index=0)
case_vals = EXAMPLE_CASES[selected_case]

# ì´ˆê¸°ê°’ ì£¼ì…
for k in REQUIRED_FEATURES:
    if f"val_{k}" not in st.session_state:
        st.session_state[f"val_{k}"] = case_vals.get(k, np.nan)

def apply_case(vals: dict):
    for k, v in vals.items():
        st.session_state[f"val_{k}"] = v

if st.button("Loading the values from the selected case..."):
    apply_case(EXAMPLE_CASES[selected_case])
    st.success(f"{selected_case} values have been loaded into the input form.")

st.subheader("ğŸ—’ï¸ Patient Variables input")
colA, colB, colC, colD, colE = st.columns(5)

with colA:
    st.session_state["val_AGE"] = st.number_input("AGE", 0, 120, int(st.session_state["val_AGE"]))
    st.session_state["val_SEX"] = st.selectbox("SEX (0=female,1=male)", [0,1], index=int(st.session_state["val_SEX"]))
    st.session_state["val_BMI"] = st.number_input("BMI", 0.0, 80.0, float(st.session_state["val_BMI"]))
    st.session_state["val_VENT_DUR"] = st.number_input("VENT_DUR (hr)", 0.0, 1000.0, float(st.session_state["val_VENT_DUR"]))
    st.session_state["val_CHF"] = st.selectbox("CHF", [0,1], index=int(st.session_state["val_CHF"]))
    st.session_state["val_CVD"] = st.selectbox("CVD", [0,1], index=int(st.session_state["val_CVD"]))
    st.session_state["val_CPD"] = st.selectbox("CPD", [0,1], index=int(st.session_state["val_CPD"]))
    st.session_state["val_CKD"] = st.selectbox("CKD", [0,1], index=int(st.session_state["val_CKD"]))

with colB:
    st.session_state["val_CLD"] = st.selectbox("CLD", [0,1], index=int(st.session_state["val_CLD"]))
    st.session_state["val_DM"] = st.selectbox("DM", [0,1], index=int(st.session_state["val_DM"]))
    st.session_state["val_ECMO"] = st.selectbox("ECMO", [0,1], index=int(st.session_state["val_ECMO"]))
    st.session_state["val_CRRT"] = st.selectbox("CRRT", [0,1], index=int(st.session_state["val_CRRT"]))
    st.session_state["val_MAP"] = st.number_input("MAP", 0.0, 200.0, float(st.session_state["val_MAP"]))
    st.session_state["val_HR"] = st.number_input("HR", 0.0, 220.0, float(st.session_state["val_HR"]))
    st.session_state["val_RR"] = st.number_input("RR", 0.0, 80.0, float(st.session_state["val_RR"]))
    st.session_state["val_BT"] = st.number_input("BT (Â°C)", value=float(case_vals.get("BT", 36.5)))

with colC:
    st.session_state["val_SPO2"] = st.number_input("SPO2 (%)", 0.0, 100.0, float(st.session_state["val_SPO2"]))
    st.session_state["val_GCS"] = st.number_input("GCS", 3.0, 15.0, float(st.session_state["val_GCS"]))
    st.session_state["val_PH"] = st.number_input("pH", 6.8, 7.8, float(st.session_state["val_PH"]), step=0.01)
    st.session_state["val_PACO2"] = st.number_input("PaCO2", 10.0, 120.0, float(st.session_state["val_PACO2"]))
    st.session_state["val_PAO2"] = st.number_input("PaO2", 10.0, 600.0, float(st.session_state["val_PAO2"]))
    st.session_state["val_HCO3"] = st.number_input("HCO3-", 5.0, 45.0, float(st.session_state["val_HCO3"]))
    st.session_state["val_LACTATE"] = st.number_input("Lactate", 0.0, 20.0, float(st.session_state["val_LACTATE"]))

with colD:
    st.session_state["val_WBC"] = st.number_input("WBC", 0.0, 100.0, float(st.session_state["val_WBC"]))
    st.session_state["val_HB"] = st.number_input("Hb", 0.0, 25.0, float(st.session_state["val_HB"]))
    st.session_state["val_PLT"] = st.number_input("PLT", 0.0, 1000.0, float(st.session_state["val_PLT"]))
    st.session_state["val_SODIUM"] = st.number_input("Na", 100.0, 180.0, float(st.session_state["val_SODIUM"]))
    st.session_state["val_POTASSIUM"] = st.number_input("K", 1.5, 8.0, float(st.session_state["val_POTASSIUM"]))
    st.session_state["val_CHLORIDE"] = st.number_input("Cl", 70.0, 140.0, float(st.session_state["val_CHLORIDE"]))
    st.session_state["val_BUN"] = st.number_input("BUN", 0.0, 200.0, float(st.session_state["val_BUN"]))

with colE:
    st.session_state["val_CR"] = st.number_input("Cr", 0.0, 20.0, float(st.session_state["val_CR"]))
    st.session_state["val_PT"] = st.number_input("PT", 5.0, 30.0, float(st.session_state["val_PT"]))
    st.session_state["val_FIO2"] = st.number_input("FiO2 (%)", 21.0, 100.0, float(st.session_state["val_FIO2"]))
    st.session_state["val_PEEP"] = st.number_input("PEEP", 0.0, 30.0, float(st.session_state["val_PEEP"]))
    st.session_state["val_PPLAT"] = st.number_input("Pplat", 0.0, 60.0, float(st.session_state["val_PPLAT"]))
    st.session_state["val_TV"] = st.number_input("TV (mL)", 0.0, 1500.0, float(st.session_state["val_TV"]))

st.subheader("â¡ï¸ Generate Prediction & Report")
run = st.button("Check Prediction Results & Report")

if run:
    # 1) ì…ë ¥ DF(ë² ì´ìŠ¤ë¼ì¸)
    patient_input = {k: float(st.session_state[f"val_{k}"]) if not np.isnan(st.session_state[f"val_{k}"]) else np.nan
                     for k in REQUIRED_FEATURES}
    df_base = _df_from_patient_input(patient_input)

    # 2) ì˜¨í†¨ë¡œì§€ íƒœê¹… â†’ ëª¨ë¸/UI ë‘˜ ë‹¤ ì‚¬ìš©
    with st.spinner("ğŸ¤– LLM agent is tagging Ontologies..."):
        try:
            if USE_LLM and OPENAI_API_KEY:
                client = build_openai_client()
                ontology_json = llm_tag_ontology(client, df_base)
            else:
                ontology_json = rule_based_ontology(df_base)
        except Exception as e:
            st.warning(f"LLM íƒœê¹… ì‹¤íŒ¨. ë£° ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. ({e})")
            ontology_json = rule_based_ontology(df_base)

    # âœ… ëª¨ë¸ ì…ë ¥/í‘œì‹œ ëª¨ë‘ ì˜¨í†¨ë¡œì§€ í¬í•¨
    df_with_onto = attach_ontology_features(df_base.copy(), ontology_json)

    # 3) ëª¨ë¸ ë¡œë“œ & ì˜ˆì¸¡ (ì˜¨í†¨ë¡œì§€ í¬í•¨ ì…ë ¥)
    with st.spinner("ğŸ¤– Predicting with Random Forest..."):
        try:
            model = load_model(FIXED_MODEL_PATH)
            expected_cols = get_expected_model_features(model, fallback_cols=ALL_FEATURES_FALLBACK)
            df_model = df_with_onto.reindex(columns=expected_cols)
            pred = run_predict(model, df_model)
        except Exception as e:
            st.error(f"ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            st.stop()

    # 4) SHAP (ì˜¨í†¨ë¡œì§€ í¬í•¨ ì…ë ¥ ê¸°ì¤€)
    with st.spinner("SHAP calculation in progress..."):
        try:
            shap_exp = compute_shap(model, df_model)
        except Exception as e:
            st.warning(f"SHAP ê³„ì‚° ì‹¤íŒ¨: {e}")
            shap_exp = {"feature_importance": {}, "top_risk_factors": []}

    # 5) ë ˆí¬íŠ¸
    report_text = None
    if USE_LLM and OPENAI_API_KEY:
        with st.spinner("ğŸ«¶ Generating explanation report for guardians..."):
            try:
                client = build_openai_client()
                report_text = llm_generate_report(client, patient_input, pred, shap_exp, ontology_json)
            except Exception as e:
                st.warning(f"Report generation failed: {e}")
                report_text = None

    # ê²°ê³¼ í‘œì‹œ
    st.success("Complete!")

    col1, col2 = st.columns([1,1])
    with col1:
        st.markdown("### Prediction Results")
        st.metric("Extubation Failure Probability", f"{pred['probability']*100:.1f}%")
        st.metric("Predicted Class", pred["class_label"])

        st.markdown("### Ontology Tagging Results")
        pretty_df = ontology_pretty_table(ontology_json)
        st.dataframe(pretty_df, hide_index=True, use_container_width=True)

    with col2:
        st.markdown("### Top Risk Factors (SHAP | Absolute Top 5)")
        if "top_risk_factors" in shap_exp and shap_exp["top_risk_factors"]:
            rows = []
            for name, abs_imp in shap_exp["top_risk_factors"]:
                sign = shap_exp["feature_importance"].get(name, 0.0)
                direction = "â†‘ ìœ„í—˜ ì¦ê°€" if sign > 0 else "â†“ ìœ„í—˜ ê°ì†Œ"
                rows.append({"feature": name, "abs_importance": round(abs_imp, 5), "direction": direction})
            st.dataframe(pd.DataFrame(rows), hide_index=True, use_container_width=True)
        else:
            st.write("ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ë³´í˜¸ì ì„¤ëª…ìš© ë ˆí¬íŠ¸ (í¸ì§‘/ë‹¤ìš´ë¡œë“œ)
    st.markdown("### Explanation Report for Guardians")
    if report_text:
        if "report_text" not in st.session_state or not st.session_state.get("report_text"):
            st.session_state.report_text = report_text
        st.session_state.report_text = st.text_area(
            "Generated Report (Editable)",
            value=st.session_state.report_text,
            height=420,
            help="í•„ìš” ì‹œ ë¬¸êµ¬ë¥¼ ìˆ˜ì •í•˜ì—¬ ë³´í˜¸ì ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì— í™œìš©í•˜ì„¸ìš”."
        )
        st.download_button(
            label="Download Report .txt",
            data=st.session_state.report_text,
            file_name=f"extubation_report_{datetime.now().strftime('%Y%m%d_%H%M')}.txt",
            mime="text/plain",
            use_container_width=True
        )
    else:
        st.info("Report has not been generated. Please run the prediction above to generate it.")

    # â–¶ ëª¨ë¸ ì…ë ¥ ì „ì²´ Feature (ì˜¨í†¨ë¡œì§€ í¬í•¨)
    st.markdown("### Every Features")
    # ë³´ê¸° ì¢‹ê²Œ: ëª¨ë¸ ì…ë ¥ í”¼ì²˜ + ì˜¨í†¨ë¡œì§€ ë¼ë²¨/ì„¤ëª… í•©ì³ì„œ ë³´ì—¬ì£¼ê¸°
    labels, desc = _ontology_label_maps()
    onto_row = ontology_json["patients"][0]
    df_display = df_model.T.rename(columns={0: "Value"})
    sep = pd.DataFrame([[""]], index=[" "], columns=["Value"])
    onto_rows = []
    for key in ONTO_FEATURES:
        onto_rows.append({
            "Feature": key,
            "íƒœê¹…ê²°ê³¼(0/1)": int(onto_row.get(key, 0)),
            "ì˜¨í†¨ë¡œì§€_íŠ¹ì„±": labels.get(key, key)
        })
    onto_df = pd.DataFrame(onto_rows).set_index("Feature")
    df_display = pd.concat([df_display, sep, onto_df], axis=0)
    st.dataframe(df_display, use_container_width=True, height=600)

    # ì±„íŒ… ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬
    st.session_state.memory = {
        "timestamp": datetime.now().isoformat(timespec="seconds"),
        "selected_case": selected_case,
        "patient_input": patient_input,
        "prediction": pred,
        "shap_top": shap_exp.get("top_risk_factors", []),
    }

# -------------------------------
# ì‚¬ì´ë“œë°”: ì±—ë´‡
# -------------------------------
with st.sidebar:
    from pathlib import Path
    image_path = Path("image_kcd.jpg")
    if image_path.exists():
        st.image(str(image_path), use_container_width=True)

    st.header("ğŸ’¬ Chatbot Assistant for Patient Guardians")
    if OPENAI_API_KEY is None:
        st.caption("OpenAI í‚¤ê°€ ì—†ì–´ì„œ ì±„íŒ…ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. (Secretsì— OPENAI_API_KEY ì¶”ê°€)")
    else:
        context_blob = json.dumps(st.session_state.memory, ensure_ascii=False, indent=2) if st.session_state.memory else "ìµœê·¼ ì˜ˆì¸¡ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ."
        system_msg = (
            "ë‹¹ì‹ ì€ ì¤‘í™˜ìì‹¤ì— ì…ì‹¤í•œ í™˜ì ë³´í˜¸ìë¥¼ ëŒ€í•˜ëŠ” ì˜ë£Œì¸ì…ë‹ˆë‹¤. "
            "ì•„ë˜ 'ìµœê·¼ ì˜ˆì¸¡ ì»¨í…ìŠ¤íŠ¸'ë¥¼ ì°¸ê³ í•˜ì—¬ ì¹œì ˆí•˜ê³  ì‰½ê²Œ ë‹µë³€í•˜ì„¸ìš”. ì˜ë£Œì¸ì´ ì•„ë‹Œ ì‚¬ëŒë“¤ë„ ì•Œì•„ë“¤ì„ ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•˜ì„¸ìš”. ì˜ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”. \n\n"
            f"[ìµœê·¼ ì˜ˆì¸¡ ì»¨í…ìŠ¤íŠ¸]\n{context_blob}"
        )
        for m in st.session_state.chat_history:
            with st.chat_message(m["role"]):
                st.markdown(m["content"])

        user_msg = st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”")
        if user_msg:
            st.session_state.chat_history.append({"role":"user","content":user_msg})
            with st.chat_message("user"):
                st.markdown(user_msg)
            try:
                client = build_openai_client()
                messages = [{"role":"system","content":system_msg}] + st.session_state.chat_history[-20:]
                resp = client.chat.completions.create(
                    model=CHAT_MODEL,
                    messages=messages,
                    temperature=0.3
                )
                bot_text = resp.choices[0].message.content.strip()
            except Exception as e:
                bot_text = f"(ì˜¤ë¥˜) {e}"
            st.session_state.chat_history.append({"role":"assistant","content":bot_text})
            with st.chat_message("assistant"):
                st.markdown(bot_text)